{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd22764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_date\n",
      "1963-01-01    30.44\n",
      "1963-02-01    30.48\n",
      "1963-03-01    30.51\n",
      "1963-04-01    30.48\n",
      "1963-05-01    30.51\n",
      "Name: cpi, dtype: float64\n",
      "Rows after filtering: 752\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'active_listings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m vacancy_rate_monthly_diff \u001b[38;5;241m=\u001b[39m vacancy_rate_monthly\u001b[38;5;241m.\u001b[39mvacancy_rate\u001b[38;5;241m.\u001b[39mdiff()\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m    109\u001b[0m morgage_rate_monthly_diff \u001b[38;5;241m=\u001b[39m morgage_rate_monthly\u001b[38;5;241m.\u001b[39mmorgage_rate\u001b[38;5;241m.\u001b[39mdiff()\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m--> 110\u001b[0m active_listings_monthly_diff \u001b[38;5;241m=\u001b[39m \u001b[43mactive_listings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_listings\u001b[49m\u001b[38;5;241m.\u001b[39mdiff()\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m    111\u001b[0m median_house_income_monthly_diff \u001b[38;5;241m=\u001b[39m median_house_income_monthly\u001b[38;5;241m.\u001b[39mmedian_house_income\u001b[38;5;241m.\u001b[39mdiff()\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m    112\u001b[0m monthly_supply_homes_monthly_diff \u001b[38;5;241m=\u001b[39m monthly_supply_homes\u001b[38;5;241m.\u001b[39mmonthly_supply_homes\u001b[38;5;241m.\u001b[39mdiff()\u001b[38;5;241m.\u001b[39mdropna()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'active_listings'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "# Read the 'Monthly' sheet from the median_house_price file\n",
    "prices = pd.read_excel(data_path / \"median_house_price.xlsx\", sheet_name=\"Monthly\")\n",
    "# Ensure the date column is datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(prices[\"observation_date\"]):\n",
    "    prices[\"observation_date\"] = pd.to_datetime(prices[\"observation_date\"])\n",
    "\n",
    "prices.set_axis(prices[\"observation_date\"])  # index by date\n",
    "# Read CPI file and parse dates; adjust column name if different\n",
    "cpi_data = pd.read_excel(\n",
    "    data_path / \"cpi.xlsx\",\n",
    "    sheet_name=\"Monthly\",\n",
    "    parse_dates=[\"observation_date\"]  # ensure this is the actual date column name\n",
    ")\n",
    "\n",
    "# Ensure the date column is datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(cpi_data[\"observation_date\"]):\n",
    "    cpi_data[\"observation_date\"] = pd.to_datetime(cpi_data[\"observation_date\"])\n",
    "\n",
    "# Filter CPI data to start from 1963-01-01 (inclusive)\n",
    "cpi_data = cpi_data[cpi_data[\"observation_date\"] >= \"1963-01-01\"]\n",
    "\n",
    "# Extract CPI series (column named 'cpi' in the Excel file)\n",
    "cpi = cpi_data[\"cpi\"]\n",
    "\n",
    "# Optional: reset index or keep dates as index\n",
    "cpi = cpi.set_axis(cpi_data[\"observation_date\"])  # index by date if desired\n",
    "\n",
    "# Quick check\n",
    "print(cpi.head())\n",
    "print(f\"Rows after filtering: {len(cpi)}\")\n",
    "\n",
    "# Calculate real house prices\n",
    "cpi_100 = cpi / 100  # Convert CPI to index form\n",
    "real_prices = prices.price.values / cpi_100.values  # Align CPI to prices index\n",
    "\n",
    "# Read other datasets similarly and compute their monthly differences\n",
    "loans = pd.read_excel(data_path / \"loans.xlsx\", sheet_name=\"Monthly\")\n",
    "loans = loans.set_index(loans[\"observation_date\"])\n",
    "\n",
    "net_wealth = pd.read_excel(data_path / \"net_wealth_of_top_1.xlsx\", sheet_name=\"Quarterly\")\n",
    "# trabsform to monthly with linear interpolation\n",
    "net_wealth =net_wealth.set_index(\"observation_date\")\n",
    "# net_wealth rename percentage column to 'net_wealth\n",
    "net_wealth.rename(columns={\"percentage\": \"net_wealth\"}, inplace=True)\n",
    "\n",
    "net_wealth_monthly = net_wealth.resample('ME').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "vacancy_rate = pd.read_excel(data_path / \"vacancy_rate.xlsx\", sheet_name=\"Quarterly\")\n",
    "vacancy_rate = vacancy_rate.set_index(\"observation_date\")\n",
    "vacancy_rate_monthly = vacancy_rate.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "\n",
    "\n",
    "morgage_rate = pd.read_excel(data_path / \"MORTGAGE30US.xlsx\", sheet_name=\"Weekly, Ending Thursday\")\n",
    "morgage_rate = morgage_rate.set_index(\"observation_date\")\n",
    "# change column name to 'morgage_rate'\n",
    "morgage_rate.rename(columns={\"MORTGAGE30US\": \"morgage_rate\"}, inplace=True)\n",
    "\n",
    "# convert weekly to monthly\n",
    "morgage_rate_monthly = morgage_rate.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "## Import more data\n",
    "\n",
    "pca_path = Path(\"data/PCA\")\n",
    "active_listings = pd.read_excel(pca_path / \"ACTLISCOUUS.xlsx\", sheet_name=\"Monthly\")\n",
    "active_listings.rename(columns={\"ACTLISCOUUS\": \"active_listings\"}, inplace=True)\n",
    "active_listings.set_index(\"observation_date\", inplace=True)\n",
    "\n",
    "\n",
    "median_house_income = pd.read_excel(pca_path / \"MEHOINUSA672N.xlsx\", sheet_name=\"Annual\")\n",
    "median_house_income['observation_date'] = pd.to_datetime(median_house_income['observation_date'], errors='coerce')\n",
    "median_house_income.set_index('observation_date', inplace=True)\n",
    "median_house_income.rename(columns={\"MEHOINUSA672N\": \"median_house_income\"}, inplace=True)\n",
    "median_house_income_monthly = median_house_income.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "monthly_supply_homes = pd.read_excel(pca_path / \"MSACSR.xlsx\", sheet_name=\"Monthly\")\n",
    "monthly_supply_homes.set_index(\"observation_date\", inplace=True)\n",
    "monthly_supply_homes.rename(columns={\"MSACSR\": \"monthly_supply_homes\"}, inplace=True)\n",
    "\n",
    "labor_share = pd.read_excel(pca_path / \"PRS85006173.xlsx\", sheet_name=\"Quarterly\")\n",
    "labor_share.set_index(\"observation_date\", inplace=True)\n",
    "labor_share.rename(columns={\"PRS85006173\": \"labor_share\"}, inplace=True)\n",
    "labor_share_monthly = labor_share.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "unemployment_rate = pd.read_excel(pca_path / \"UNRATE.xlsx\", sheet_name=\"Monthly\")\n",
    "unemployment_rate.set_index(\"observation_date\", inplace=True)\n",
    "unemployment_rate.rename(columns={\"UNRATE\": \"unemployment_rate\"}, inplace=True)\n",
    "\n",
    "share_net_worth_bottom_50 = pd.read_excel(pca_path / \"WFRBSB50215.xlsx\", sheet_name=\"Quarterly\")\n",
    "share_net_worth_bottom_50.set_index(\"observation_date\", inplace=True)\n",
    "share_net_worth_bottom_50.rename(columns={\"WFRBSB50215\": \"share_net_worth_bottom_50\"}, inplace=True)\n",
    "share_net_worth_bottom_50_monthly = share_net_worth_bottom_50.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "\n",
    "# Calculate monthly differences\n",
    "log_real_prices = np.log(real_prices)\n",
    "log_real_prices = pd.Series(log_real_prices, index=prices.observation_date)\n",
    "\n",
    "log_real_prices_diff = log_real_prices.diff().dropna()\n",
    "loans_monthly_diff = loans.loans.diff().dropna()\n",
    "net_wealth_monthly_diff = net_wealth_monthly.net_wealth.diff().dropna()\n",
    "vacancy_rate_monthly_diff = vacancy_rate_monthly.vacancy_rate.diff().dropna()\n",
    "morgage_rate_monthly_diff = morgage_rate_monthly.morgage_rate.diff().dropna()\n",
    "active_listings_monthly_diff = active_listings.active_listings.diff().dropna()\n",
    "median_house_income_monthly_diff = median_house_income_monthly.median_house_income.diff().dropna()\n",
    "monthly_supply_homes_monthly_diff = monthly_supply_homes.monthly_supply_homes.diff().dropna()\n",
    "labor_share_monthly_diff = labor_share_monthly.labor_share.diff().dropna()\n",
    "unemployment_rate_monthly_diff = unemployment_rate.unemployment_rate.diff().dropna()\n",
    "share_net_worth_bottom_50_monthly_diff = share_net_worth_bottom_50_monthly.share_net_worth_bottom_50.diff().dropna()    \n",
    "# Combine all variables into a single DataFrame\n",
    "# Align all series to the real_prices_diff datetime index and build the DataFrame\n",
    "idx = prices.observation_date\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'real_prices_diff': log_real_prices_diff,\n",
    "    'loans_diff': loans_monthly_diff.reindex(idx),\n",
    "    'net_wealth_diff': net_wealth_monthly_diff.reindex(idx),\n",
    "    'vacancy_rate_diff': vacancy_rate_monthly_diff.reindex(idx),\n",
    "    'morgage_rate_diff': morgage_rate_monthly_diff.reindex(idx),\n",
    "    'active_listings_diff': active_listings_monthly_diff.reindex(idx),\n",
    "    'median_house_income_diff': median_house_income_monthly_diff.reindex(idx),\n",
    "    'monthly_supply_homes_diff': monthly_supply_homes_monthly_diff.reindex(idx),\n",
    "    'labor_share_diff': labor_share_monthly_diff.reindex(idx),\n",
    "    'unemployment_rate_diff': unemployment_rate_monthly_diff.reindex(idx),\n",
    "    'share_net_worth_bottom_50_diff': share_net_worth_bottom_50_monthly_diff.reindex(idx)\n",
    "}, index=idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a85c2bad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 23\u001b[0m\n\u001b[1;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m data[[\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloans_diff\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet_wealth_diff\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munemployment_rate_diff\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshare_net_worth_bottom_50_diff\u001b[39m\u001b[38;5;124m'\u001b[39m ]]\n\u001b[1;32m     20\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal_prices_diff\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTEP 2: STANDARDIZATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2785\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2415\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2412\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2419\u001b[0m     )\n\u001b[1;32m   2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "X = data[[\n",
    "'loans_diff',\n",
    "'net_wealth_diff',\n",
    "'vacancy_rate_diff',\n",
    "'morgage_rate_diff',\n",
    "'active_listings_diff',\n",
    "'median_house_income_diff',\n",
    "'monthly_supply_homes_diff',\n",
    "'labor_share_diff',\n",
    "'unemployment_rate_diff',\n",
    "'share_net_worth_bottom_50_diff' ]]\n",
    "y = data['real_prices_diff']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: STANDARDIZATION\")\n",
    "print(\"=\" * 60)\n",
    "# Standardize features (required for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeatures standardized (mean=0, std=1)\")\n",
    "print(\"Training set shape:\", X_train_scaled.shape)\n",
    "print(\"Test set shape:\", X_test_scaled.shape)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: PCA FACTOR ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Display explained variance\n",
    "print(\"\\nExplained variance by each component:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_, 1):\n",
    "    print(f\"  PC{i}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative explained variance:\")\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "for i, var in enumerate(cumsum, 1):\n",
    "    print(f\"  PC1-PC{i}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "# Component loadings\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i}' for i in range(1, len(pca.components_) + 1)],\n",
    "    index=X.columns\n",
    ")\n",
    "print(\"\\nPCA Loadings (Component Matrix):\")\n",
    "print(loadings.round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: DETERMINE NUMBER OF FACTORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Kaiser criterion: eigenvalues > 1\n",
    "eigenvalues = pca.explained_variance_\n",
    "print(\"\\nEigenvalues:\")\n",
    "for i, ev in enumerate(eigenvalues, 1):\n",
    "    print(f\"  PC{i}: {ev:.4f}\", \"✓ (Keep)\" if ev > 1 else \"  (Drop)\")\n",
    "\n",
    "n_components_kaiser = sum(eigenvalues > 1)\n",
    "print(f\"\\nKaiser criterion suggests keeping {n_components_kaiser} component(s)\")\n",
    "\n",
    "# Choose based on variance threshold (e.g., 80%)\n",
    "variance_threshold = 0.80\n",
    "n_components_var = np.argmax(cumsum >= variance_threshold) + 1\n",
    "print(f\"To explain {variance_threshold*100}% variance: {n_components_var} component(s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: REGRESSION WITH PRINCIPAL COMPONENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the number of components that explain ~80-90% variance\n",
    "n_final = max(n_components_kaiser, n_components_var)\n",
    "print(f\"\\nUsing {n_final} principal component(s) for regression\")\n",
    "\n",
    "X_train_pca_reduced = X_train_pca[:, :n_final]\n",
    "X_test_pca_reduced = X_test_pca[:, :n_final]\n",
    "\n",
    "# Fit regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_pca_reduced, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train_pca_reduced)\n",
    "y_test_pred = model.predict(X_test_pca_reduced)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  Training R²: {train_r2:.4f}\")\n",
    "print(f\"  Test R²: {test_r2:.4f}\")\n",
    "print(f\"  Training RMSE: {train_rmse:.6f}\")\n",
    "print(f\"  Test RMSE: {test_rmse:.6f}\")\n",
    "print(f\"  Training MAE: {train_mae:.6f}\")\n",
    "print(f\"  Test MAE: {test_mae:.6f}\")\n",
    "\n",
    "print(\"\\nRegression Coefficients (for PCs):\")\n",
    "for i, coef in enumerate(model.coef_, 1):\n",
    "    print(f\"  PC{i}: {coef:.4f}\")\n",
    "print(f\"  Intercept: {model.intercept_:.6f}\")\n",
    "\n",
    "# Interpretation: Map PC coefficients back to original variables\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION: Effect on Price Changes\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nApproximate contribution of original variables:\")\n",
    "print(\"(PC coefficients × loadings)\")\n",
    "\n",
    "original_effects = np.dot(loadings.iloc[:, :n_final], model.coef_)\n",
    "for var, effect in zip(X.columns, original_effects):\n",
    "    print(f\"  {var}: {effect:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Scree plot\n",
    "axes[0, 0].bar(range(1, len(eigenvalues) + 1), eigenvalues, alpha=0.7, color='steelblue')\n",
    "axes[0, 0].axhline(y=1, color='r', linestyle='--', label='Kaiser criterion', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Principal Component', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Eigenvalue', fontsize=11)\n",
    "axes[0, 0].set_title('Scree Plot', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Cumulative variance explained\n",
    "axes[0, 1].plot(range(1, len(cumsum) + 1), cumsum, marker='o', linewidth=2, \n",
    "                color='steelblue', markersize=8)\n",
    "axes[0, 1].axhline(y=0.8, color='r', linestyle='--', label='80% variance', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Number of Components', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Cumulative Explained Variance', fontsize=11)\n",
    "axes[0, 1].set_title('Cumulative Variance Explained', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylim([0, 1.05])\n",
    "\n",
    "# 3. Component loadings heatmap\n",
    "sns.heatmap(loadings.iloc[:, :n_final], annot=True, cmap='RdBu_r', center=0, \n",
    "            ax=axes[1, 0], cbar_kws={'label': 'Loading'}, fmt='.3f',\n",
    "            vmin=-1, vmax=1, linewidths=0.5)\n",
    "axes[1, 0].set_title('PCA Loadings Heatmap', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Principal Component', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Original Variable (Differenced)', fontsize=11)\n",
    "\n",
    "# 4. Actual vs Predicted\n",
    "axes[1, 1].scatter(y_test, y_test_pred, alpha=0.6, color='steelblue', \n",
    "                   edgecolors='black', s=50)\n",
    "min_val = min(y_test.min(), y_test_pred.min())\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, \n",
    "                label='Perfect prediction')\n",
    "axes[1, 1].set_xlabel('Actual Price Change (Log Diff)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Predicted Price Change (Log Diff)', fontsize=11)\n",
    "axes[1, 1].set_title(f'Actual vs Predicted (Test Set)\\nR² = {test_r2:.4f}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_analysis_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualizations saved as 'pca_analysis_results.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNote: You're modeling CHANGES in log prices using CHANGES in predictors.\")\n",
    "print(\"This is appropriate for non-stationary time series data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
