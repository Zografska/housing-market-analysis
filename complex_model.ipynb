{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd22764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_date\n",
      "1963-01-01    30.44\n",
      "1963-02-01    30.48\n",
      "1963-03-01    30.51\n",
      "1963-04-01    30.48\n",
      "1963-05-01    30.51\n",
      "Name: cpi, dtype: float64\n",
      "Rows after filtering: 752\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (751, 1) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 135\u001b[0m\n\u001b[1;32m    132\u001b[0m idx \u001b[38;5;241m=\u001b[39m log_real_prices_diff\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Combine all variables into a single DataFrame\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreal_prices_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_real_prices_diff\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloans_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_loans_monthly_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnet_wealth_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_wealth_monthly_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvacancy_rate_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvacancy_rate_monthly_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmortgage_rate_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmortgage_rate_monthly_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 'active_listings_diff': active_listings_monthly_diff.reindex(idx),\u001b[39;49;00m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedian_house_income_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmedian_house_income_monthly_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonthly_supply_homes_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonthly_supply_homes_monthly_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabor_share_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabor_share_monthly_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munemployment_rate_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43munemployment_rate_monthly_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetrended_share_net_worth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_net_worth_bottom_50_monthly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Check for missing values\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     arrays, refs \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:629\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    626\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[1;32m    627\u001b[0m     val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m--> 629\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m com\u001b[38;5;241m.\u001b[39mrequire_length_match(val, index)\n\u001b[1;32m    631\u001b[0m refs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/construction.py:633\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m     _sanitize_non_ordered(data)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/construction.py:659\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    656\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n\u001b[1;32m    657\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[0;32m--> 659\u001b[0m subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[1;32m    663\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/construction.py:718\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[0;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m     )\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     result \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (751, 1) instead"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "# Read the 'Monthly' sheet from the median_house_price file\n",
    "prices = pd.read_excel(data_path / \"median_house_price.xlsx\", sheet_name=\"Monthly\")\n",
    "# Ensure the date column is datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(prices[\"observation_date\"]):\n",
    "    prices[\"observation_date\"] = pd.to_datetime(prices[\"observation_date\"])\n",
    "\n",
    "# FIX: Actually set the index properly\n",
    "prices = prices.set_index(\"observation_date\")\n",
    "\n",
    "# Read CPI file and parse dates; adjust column name if different\n",
    "cpi_data = pd.read_excel(\n",
    "    data_path / \"cpi.xlsx\",\n",
    "    sheet_name=\"Monthly\",\n",
    "    parse_dates=[\"observation_date\"]\n",
    ")\n",
    "\n",
    "# Ensure the date column is datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(cpi_data[\"observation_date\"]):\n",
    "    cpi_data[\"observation_date\"] = pd.to_datetime(cpi_data[\"observation_date\"])\n",
    "\n",
    "# Filter CPI data to start from 1963-01-01 (inclusive)\n",
    "cpi_data = cpi_data[cpi_data[\"observation_date\"] >= \"1963-01-01\"]\n",
    "\n",
    "# Extract CPI series and set index\n",
    "cpi = cpi_data.set_index(\"observation_date\")[\"cpi\"]\n",
    "\n",
    "# Quick check\n",
    "print(cpi.head())\n",
    "print(f\"Rows after filtering: {len(cpi)}\")\n",
    "\n",
    "# Calculate real house prices\n",
    "cpi_100 = cpi / 100  # Convert CPI to index form\n",
    "real_prices = prices.price / cpi_100  # Now both are properly indexed\n",
    "\n",
    "# Read other datasets similarly and compute their monthly differences\n",
    "loans = pd.read_excel(data_path / \"loans.xlsx\", sheet_name=\"Monthly\")\n",
    "loans = loans.set_index(\"observation_date\")\n",
    "log_loans = np.log(loans.loans)\n",
    "log_loans = log_loans.to_frame(name='loans').set_index(loans.index)\n",
    "\n",
    "\n",
    "\n",
    "net_wealth = pd.read_excel(data_path / \"net_wealth_of_top_1.xlsx\", sheet_name=\"Quarterly\")\n",
    "net_wealth = net_wealth.set_index(\"observation_date\")\n",
    "net_wealth.rename(columns={\"percentage\": \"net_wealth\"}, inplace=True)\n",
    "net_wealth_monthly = net_wealth.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "vacancy_rate = pd.read_excel(data_path / \"vacancy_rate.xlsx\", sheet_name=\"Quarterly\")\n",
    "vacancy_rate = vacancy_rate.set_index(\"observation_date\")\n",
    "vacancy_rate_monthly = vacancy_rate.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "mortgage_rate = pd.read_excel(data_path / \"MORTGAGE30US.xlsx\", sheet_name=\"Weekly, Ending Thursday\")\n",
    "mortgage_rate = mortgage_rate.set_index(\"observation_date\")\n",
    "mortgage_rate.rename(columns={\"MORTGAGE30US\": \"mortgage_rate\"}, inplace=True)\n",
    "mortgage_rate_monthly = mortgage_rate.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "## Import more data\n",
    "pca_path = Path(\"data/PCA\")\n",
    "\n",
    "active_listings = pd.read_excel(pca_path / \"ACTLISCOUUS.xlsx\", sheet_name=\"Monthly\")\n",
    "active_listings.rename(columns={\"ACTLISCOUUS\": \"active_listings\"}, inplace=True)\n",
    "active_listings.set_index(\"observation_date\", inplace=True)\n",
    "\n",
    "median_house_income = pd.read_excel(pca_path / \"MEHOINUSA672N.xlsx\", sheet_name=\"Annual\")\n",
    "median_house_income['observation_date'] = pd.to_datetime(median_house_income['observation_date'], errors='coerce')\n",
    "median_house_income.set_index('observation_date', inplace=True)\n",
    "median_house_income.rename(columns={\"MEHOINUSA672N\": \"median_house_income\"}, inplace=True)\n",
    "median_house_income_monthly = median_house_income.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "monthly_supply_homes = pd.read_excel(pca_path / \"MSACSR.xlsx\", sheet_name=\"Monthly\")\n",
    "monthly_supply_homes.set_index(\"observation_date\", inplace=True)\n",
    "monthly_supply_homes.rename(columns={\"MSACSR\": \"monthly_supply_homes\"}, inplace=True)\n",
    "\n",
    "labor_share = pd.read_excel(pca_path / \"PRS85006173.xlsx\", sheet_name=\"Quarterly\")\n",
    "labor_share.set_index(\"observation_date\", inplace=True)\n",
    "labor_share.rename(columns={\"PRS85006173\": \"labor_share\"}, inplace=True)\n",
    "labor_share_monthly = labor_share.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "unemployment_rate = pd.read_excel(pca_path / \"UNRATE.xlsx\", sheet_name=\"Monthly\")\n",
    "unemployment_rate.set_index(\"observation_date\", inplace=True)\n",
    "unemployment_rate.rename(columns={\"UNRATE\": \"unemployment_rate\"}, inplace=True)\n",
    "\n",
    "share_net_worth_bottom_50 = pd.read_excel(pca_path / \"WFRBSB50215.xlsx\", sheet_name=\"Quarterly\")\n",
    "share_net_worth_bottom_50.set_index(\"observation_date\", inplace=True)\n",
    "share_net_worth_bottom_50.rename(columns={\"WFRBSB50215\": \"share_net_worth_bottom_50\"}, inplace=True)\n",
    "share_net_worth_bottom_50_monthly = share_net_worth_bottom_50.resample('MS').interpolate(method='linear').ffill().bfill()\n",
    "log_share_net_worth_bottom_50_monthly = np.log(share_net_worth_bottom_50_monthly.share_net_worth_bottom_50)\n",
    "log_share_net_worth_bottom_50_monthly = log_share_net_worth_bottom_50_monthly.to_frame(name='share_net_worth_bottom_50').set_index(share_net_worth_bottom_50_monthly.index)\n",
    "\n",
    "# Calculate monthly differences\n",
    "log_real_prices = np.log(real_prices)\n",
    "log_real_prices_diff = log_real_prices.diff().dropna()\n",
    "\n",
    "log_loans_monthly_diff = log_loans.loans.diff().dropna()\n",
    "\n",
    "time = np.arange(len(log_loans_monthly_diff))  # assuming loans is a time series\n",
    "X = sm.add_constant(time)  # Add constant to model for intercept\n",
    "\n",
    "# Fit linear regression model\n",
    "model = sm.OLS(log_loans_monthly_diff, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Detrended data (subtract the fitted trend)\n",
    "detrended_loans = log_loans_monthly_diff - results.fittedvalues\n",
    "\n",
    "\n",
    "net_wealth_monthly_diff = net_wealth_monthly.net_wealth.diff().dropna()\n",
    "vacancy_rate_monthly_diff = vacancy_rate_monthly.vacancy_rate.diff().dropna()\n",
    "mortgage_rate_monthly_diff = mortgage_rate_monthly.mortgage_rate.diff().dropna()\n",
    "active_listings_monthly_diff = active_listings.active_listings.diff().dropna()\n",
    "median_house_income_monthly_diff = median_house_income_monthly.median_house_income.diff().dropna()\n",
    "monthly_supply_homes_monthly_diff = monthly_supply_homes.monthly_supply_homes.diff().dropna()\n",
    "labor_share_monthly_diff = labor_share_monthly.labor_share.diff().dropna()\n",
    "unemployment_rate_monthly_diff = unemployment_rate.unemployment_rate.diff().dropna()\n",
    "log_share_net_worth_bottom_50_monthly_diff = log_share_net_worth_bottom_50_monthly.share_net_worth_bottom_50.diff().dropna()\n",
    "time = np.arange(len(log_share_net_worth_bottom_50_monthly_diff))  # assuming loans is a time series\n",
    "X = sm.add_constant(time)  # Add constant to model for intercept\n",
    "\n",
    "# Fit linear regression model\n",
    "model = sm.OLS(log_share_net_worth_bottom_50_monthly_diff, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Detrended data (subtract the fitted trend)\n",
    "detrended_share_net_worth = log_share_net_worth_bottom_50_monthly_diff - results.fittedvalues\n",
    "# FIX: Use the actual index from log_real_prices_diff\n",
    "idx = log_real_prices_diff.index\n",
    "\n",
    "# Combine all variables into a single DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'real_prices_diff': log_real_prices_diff,\n",
    "    'detrended_loans_diff': detrended_loans.reindex(idx),\n",
    "    'net_wealth_diff': net_wealth_monthly_diff.reindex(idx),\n",
    "    'vacancy_rate_diff': vacancy_rate_monthly_diff.reindex(idx),\n",
    "    'mortgage_rate_diff': mortgage_rate_monthly_diff.reindex(idx),\n",
    "    # 'active_listings_diff': active_listings_monthly_diff.reindex(idx),\n",
    "    'median_house_income_diff': median_house_income_monthly_diff.reindex(idx),\n",
    "    'monthly_supply_homes_diff': monthly_supply_homes_monthly_diff.reindex(idx),\n",
    "    'labor_share_diff': labor_share_monthly_diff.reindex(idx),\n",
    "    'unemployment_rate_diff': unemployment_rate_monthly_diff.reindex(idx),\n",
    "    'detrended_share_net_worth': detrended_share_net_worth.reindex(idx)\n",
    "}, index=idx)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "print(f\"\\nData shape: {data.shape}\")\n",
    "print(f\"\\nDate range: {data.index.min()} to {data.index.max()}\")\n",
    "# remove rows with any missing values\n",
    "data = data.dropna()\n",
    "print(f\"\\nData shape after dropping missing values: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614435c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Fit VAR model and test different lag orders\n",
    "model = VAR(data)\n",
    "lag_order_results = model.select_order(maxlags=12)\n",
    "print(lag_order_results.summary())\n",
    "\n",
    "# # This will show AIC, BIC, HQIC for different lags\n",
    "# # Lower values = better\n",
    "# optimal_lag = lag_order_results.aic  # or .bic, .hqic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import ccf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check cross-correlations between each predictor and target\n",
    "predictors = [col for col in data.columns if col != 'real_prices_diff']\n",
    "\n",
    "# test stationarity for each predictor\n",
    "for predictor in predictors:\n",
    "    clean_data = data[['real_prices_diff', predictor]].dropna()\n",
    "    adf_result = adfuller(clean_data[predictor])\n",
    "    print(f\"ADF Statistic for {predictor}: {adf_result[0]}\")\n",
    "    print(f\"p-value: {adf_result[1]}\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print('Critical Value (%s): %.3f' % (key, value))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10868b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for predictor in predictors:\n",
    "    clean_data = data[['real_prices_diff', predictor]].dropna()\n",
    "    correlations = ccf(clean_data[predictor], clean_data['real_prices_diff'])[:13]\n",
    "    \n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.stem(range(len(correlations)), correlations)\n",
    "    plt.title(f'Cross-correlation: {predictor} -> real_prices_diff')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print significant lags (rule of thumb: |corr| > 2/sqrt(n))\n",
    "    threshold = 2/np.sqrt(len(clean_data))\n",
    "    significant_lags = [i for i, corr in enumerate(correlations) if abs(corr) > threshold]\n",
    "    print(f\"{predictor}: Significant lags at {significant_lags}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Granger causality tests\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Test if lagged values of each predictor help predict target\n",
    "for predictor in predictors:\n",
    "    test_data = data[['real_prices_diff', predictor]].dropna()\n",
    "    print(f\"\\n--- Granger Causality: {predictor} -> real_prices_diff ---\")\n",
    "    try:\n",
    "        results = grangercausalitytests(test_data, maxlag=12, verbose=True)\n",
    "    except:\n",
    "        print(\"Test failed (possibly due to insufficient data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create lagged features\n",
    "def create_lagged_features(df, target_col, max_lag=12):\n",
    "    df_lagged = df.copy()\n",
    "    predictors = [col for col in df.columns if col != target_col]\n",
    "    \n",
    "    for col in predictors:\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            df_lagged[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "    \n",
    "    return df_lagged.dropna()\n",
    "\n",
    "# Create features\n",
    "df_with_lags = create_lagged_features(data, 'real_prices_diff', max_lag=12)\n",
    "\n",
    "X = df_with_lags.drop('real_prices_diff', axis=1)\n",
    "y = df_with_lags['real_prices_diff']\n",
    "\n",
    "# Use Random Forest feature importance\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5effbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import ccf, grangercausalitytests\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your data DataFrame is already defined\n",
    "# data = pd.DataFrame({...})\n",
    "\n",
    "def find_significant_lags(data, target_col='real_prices_diff', max_lag=12):\n",
    "    \"\"\"\n",
    "    Find significant lags using practical methods that work well with differenced data\n",
    "    Focus on: Cross-correlation, Individual regressions, and PACF\n",
    "    \"\"\"\n",
    "    predictors = [col for col in data.columns if col != target_col]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FINDING SIGNIFICANT LAGS FOR PREDICTORS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    message = \"\"\n",
    "    \n",
    "    for predictor in predictors:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Analyzing: {predictor}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Clean data\n",
    "        clean_data = data[[target_col, predictor]].dropna()\n",
    "        n = len(clean_data)\n",
    "        \n",
    "        if n < max_lag + 10:\n",
    "            print(f\"⚠️  Insufficient data (n={n}). Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Method 1: Cross-Correlation Function (Most reliable for differenced data)\n",
    "        print(\"\\n--- Cross-Correlation Analysis ---\")\n",
    "        ccf_values = ccf(clean_data[predictor], clean_data[target_col], adjusted=False)[:max_lag+1]\n",
    "        \n",
    "        # Significance threshold: ±1.96/sqrt(n) for 95% confidence\n",
    "        threshold = 1.96 / np.sqrt(n)\n",
    "        \n",
    "        significant_lags_ccf = []\n",
    "        for lag in range(max_lag + 1):\n",
    "            if abs(ccf_values[lag]) > threshold:\n",
    "                significant_lags_ccf.append(lag)\n",
    "                sig_marker = '***' if abs(ccf_values[lag]) > 2*threshold else '**'\n",
    "                print(f\"  Lag {lag}: {ccf_values[lag]:.4f} {sig_marker}\")\n",
    "        \n",
    "        if not significant_lags_ccf:\n",
    "            print(\"  No significant lags found\")\n",
    "        \n",
    "        # Method 2: Individual Lag Regressions (More powerful than Granger for single predictors)\n",
    "        print(\"\\n--- Individual Lag Regression t-tests ---\")\n",
    "        from statsmodels.api import OLS, add_constant\n",
    "        \n",
    "        significant_lags_regression = []\n",
    "        regression_results = {}\n",
    "        \n",
    "        for lag in range(1, max_lag + 1):\n",
    "            # Create lagged predictor\n",
    "            lagged_data = pd.DataFrame({\n",
    "                'y': clean_data[target_col].values[lag:],\n",
    "                'x_lag': clean_data[predictor].values[:-lag]\n",
    "            })\n",
    "            \n",
    "            X = add_constant(lagged_data['x_lag'])\n",
    "            y = lagged_data['y']\n",
    "            \n",
    "            model = OLS(y, X).fit()\n",
    "            p_value = model.pvalues['x_lag']\n",
    "            coef = model.params['x_lag']\n",
    "            t_stat = model.tvalues['x_lag']\n",
    "            regression_results[lag] = {'coef': coef, 'p_value': p_value, 't_stat': t_stat}\n",
    "            \n",
    "            if p_value < 0.1:  # Using 10% threshold for screening\n",
    "                significant_lags_regression.append(lag)\n",
    "                sig_marker = '***' if p_value < 0.01 else '**' if p_value < 0.05 else '*'\n",
    "                print(f\"  Lag {lag}: coef = {coef:.6f}, t-stat = {t_stat:.3f}, p = {p_value:.4f} {sig_marker}\")\n",
    "                message += f\"Predictor {predictor} has significant lag {lag} (p={p_value:.4f})\\n\"\n",
    "        \n",
    "        if not significant_lags_regression:\n",
    "            print(\"  No significant lags found (p < 0.1)\")\n",
    "        \n",
    "        # Method 3: Stepwise regression (test multiple lags together)\n",
    "        print(\"\\n--- Stepwise Regression (controlling for other lags) ---\")\n",
    "        from statsmodels.api import OLS, add_constant\n",
    "        \n",
    "        # Test all lags together\n",
    "        lag_data = pd.DataFrame({'y': clean_data[target_col].values[max_lag:]})\n",
    "        \n",
    "        for lag in range(1, max_lag + 1):\n",
    "            lag_data[f'x_lag{lag}'] = clean_data[predictor].values[max_lag-lag:-lag]\n",
    "        \n",
    "        X_full = add_constant(lag_data.drop('y', axis=1))\n",
    "        y_full = lag_data['y']\n",
    "        \n",
    "        model_full = OLS(y_full, X_full).fit()\n",
    "        \n",
    "        print(f\"\\n  Full model R² = {model_full.rsquared:.4f}, Adj. R² = {model_full.rsquared_adj:.4f}\")\n",
    "        print(\"  Significant lags in joint model:\")\n",
    "        \n",
    "        joint_significant = []\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            col_name = f'x_lag{lag}'\n",
    "            if col_name in model_full.pvalues.index:\n",
    "                p_val = model_full.pvalues[col_name]\n",
    "                if p_val < 0.1:\n",
    "                    joint_significant.append(lag)\n",
    "                    coef = model_full.params[col_name]\n",
    "                    sig_marker = '***' if p_val < 0.01 else '**' if p_val < 0.05 else '*'\n",
    "                    print(f\"    Lag {lag}: coef = {coef:.6f}, p = {p_val:.4f} {sig_marker}\")\n",
    "                    message += f\"Predictor {predictor} has significant lag {lag} (p={p_val:.4f})\\n\"\n",
    "        \n",
    "        if not joint_significant:\n",
    "            print(\"    No significant lags in joint model\")\n",
    "        \n",
    "        # Store results\n",
    "        results[predictor] = {\n",
    "            'ccf_values': ccf_values,\n",
    "            'ccf_threshold': threshold,\n",
    "            'significant_lags_ccf': significant_lags_ccf,\n",
    "            'significant_lags_regression': significant_lags_regression,\n",
    "            'regression_results': regression_results,\n",
    "            'joint_significant': joint_significant,\n",
    "            'full_model_r2': model_full.rsquared_adj,\n",
    "            'n_obs': n\n",
    "        }\n",
    "        \n",
    "        # Summary recommendation\n",
    "        print(\"\\n--- RECOMMENDATION ---\")\n",
    "        # Combine evidence from different methods\n",
    "        all_significant = set(significant_lags_ccf) | set(significant_lags_regression) | set(joint_significant)\n",
    "        \n",
    "        if all_significant:\n",
    "            recommended_lags = sorted(list(all_significant))\n",
    "            print(f\"  ✓ Recommended lags to include: {recommended_lags}\")\n",
    "            message += f\"Predictor {predictor} has significant lags: {recommended_lags}\\n\"\n",
    "            \n",
    "            # Categorize by strength of evidence\n",
    "            strong_lags = set(significant_lags_ccf) & set(significant_lags_regression)\n",
    "            if strong_lags:\n",
    "                print(f\"  ✓✓ Strongest evidence for lags: {sorted(list(strong_lags))}\")\n",
    "                message += f\"Strong evidence for lags: {sorted(list(strong_lags))}\\n\"\n",
    "        else:\n",
    "            print(f\"  ✗ No significant lags found. Consider excluding this predictor or using contemporaneous value only.\")\n",
    "    \n",
    "    print(message)\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_lag_analysis(results, data, target_col='real_prices_diff'):\n",
    "    \"\"\"\n",
    "    Create visualizations of lag relationships\n",
    "    \"\"\"\n",
    "    predictors = list(results.keys())\n",
    "    n_predictors = len(predictors)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_predictors, 2, figsize=(14, 4*n_predictors))\n",
    "    if n_predictors == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, predictor in enumerate(predictors):\n",
    "        res = results[predictor]\n",
    "        max_lag = len(res['ccf_values']) - 1\n",
    "        lags = range(max_lag + 1)\n",
    "        \n",
    "        # Plot 1: Cross-correlation\n",
    "        ax1 = axes[idx, 0]\n",
    "        ax1.stem(lags, res['ccf_values'], basefmt=' ')\n",
    "        ax1.axhline(y=res['ccf_threshold'], color='r', linestyle='--', label='95% CI')\n",
    "        ax1.axhline(y=-res['ccf_threshold'], color='r', linestyle='--')\n",
    "        ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "        ax1.set_xlabel('Lag')\n",
    "        ax1.set_ylabel('Cross-correlation')\n",
    "        ax1.set_title(f'{predictor}\\nCross-correlation with {target_col}')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight significant lags\n",
    "        for lag in res['significant_lags_ccf']:\n",
    "            ax1.axvspan(lag-0.3, lag+0.3, alpha=0.2, color='green')\n",
    "        \n",
    "        # Plot 2: Regression coefficients and p-values\n",
    "        ax2 = axes[idx, 1]\n",
    "        reg_results = res['regression_results']\n",
    "        lags_reg = list(reg_results.keys())\n",
    "        coefs = [reg_results[lag]['coef'] for lag in lags_reg]\n",
    "        p_values = [reg_results[lag]['p_value'] for lag in lags_reg]\n",
    "        \n",
    "        # Bar plot of coefficients with color based on significance\n",
    "        colors = ['green' if p < 0.05 else 'orange' if p < 0.1 else 'gray' \n",
    "                  for p in p_values]\n",
    "        ax2.bar(lags_reg, coefs, color=colors, alpha=0.6)\n",
    "        ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "        ax2.set_xlabel('Lag')\n",
    "        ax2.set_ylabel('Regression Coefficient')\n",
    "        ax2.set_title(f'{predictor}\\nIndividual Lag Coefficients\\n(Green: p<0.05, Orange: p<0.1)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_optimal_lag_model(data, results, target_col='real_prices_diff', \n",
    "                             min_lag_evidence=1):\n",
    "    \"\"\"\n",
    "    Create a regression model using the identified significant lags\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_lag_evidence : int\n",
    "        Minimum number of methods that must agree on lag significance (1-3)\n",
    "    \"\"\"\n",
    "    from statsmodels.api import OLS, add_constant\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BUILDING OPTIMAL LAG MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Collect all recommended lags\n",
    "    model_features = {}\n",
    "    \n",
    "    for predictor, res in results.items():\n",
    "        # Count evidence for each lag\n",
    "        all_lags = set(res['significant_lags_ccf']) | \\\n",
    "                   set(res['significant_lags_regression']) | \\\n",
    "                   set(res.get('joint_significant', []))\n",
    "        \n",
    "        if all_lags:\n",
    "            # For each lag, count how many methods found it significant\n",
    "            lag_evidence = {}\n",
    "            for lag in all_lags:\n",
    "                evidence = 0\n",
    "                if lag in res['significant_lags_ccf']:\n",
    "                    evidence += 1\n",
    "                if lag in res['significant_lags_regression']:\n",
    "                    evidence += 1\n",
    "                if lag in res.get('joint_significant', []):\n",
    "                    evidence += 1\n",
    "                lag_evidence[lag] = evidence\n",
    "            \n",
    "            # Keep lags with sufficient evidence\n",
    "            selected_lags = [lag for lag, ev in lag_evidence.items() \n",
    "                           if ev >= min_lag_evidence]\n",
    "            \n",
    "            if selected_lags:\n",
    "                model_features[predictor] = sorted(selected_lags)\n",
    "                print(f\"\\n{predictor}:\")\n",
    "                print(f\"  Selected lags: {selected_lags}\")\n",
    "    \n",
    "    # Build the model\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Building regression model...\")\n",
    "    \n",
    "    # Determine maximum lag needed\n",
    "    max_lag_needed = max([max(lags) for lags in model_features.values()])\n",
    "    \n",
    "    # Create lagged features\n",
    "    model_data = pd.DataFrame({target_col: data[target_col].values[max_lag_needed:]})\n",
    "    \n",
    "    for predictor, lags in model_features.items():\n",
    "        for lag in lags:\n",
    "            col_name = f'{predictor}_lag{lag}'\n",
    "            model_data[col_name] = data[predictor].values[max_lag_needed-lag:-lag]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    model_data = model_data.dropna()\n",
    "    \n",
    "    print(f\"\\nModel data shape: {model_data.shape}\")\n",
    "    print(f\"Number of features: {model_data.shape[1] - 1}\")\n",
    "    print(f\"Number of observations: {model_data.shape[0]}\")\n",
    "    \n",
    "    # Fit model\n",
    "    y = model_data[target_col]\n",
    "    X = add_constant(model_data.drop(target_col, axis=1))\n",
    "    \n",
    "    model = OLS(y, X).fit()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model, model_data, model_features\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "results = find_significant_lags(data, max_lag=12)\n",
    "plot_lag_analysis(results, data)\n",
    "model, model_data, features = create_optimal_lag_model(data, results, min_lag_evidence=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import ccf, grangercausalitytests\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your data DataFrame is already defined\n",
    "# data = pd.DataFrame({...})\n",
    "\n",
    "def find_significant_lags(data, target_col='real_prices_diff', max_lag=12):\n",
    "    \"\"\"\n",
    "    Find significant lags using practical methods that work well with differenced data\n",
    "    Focus on: Cross-correlation, Individual regressions, and PACF\n",
    "    \"\"\"\n",
    "    predictors = [col for col in data.columns if col != target_col]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FINDING SIGNIFICANT LAGS FOR PREDICTORS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for predictor in predictors:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Analyzing: {predictor}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Clean data\n",
    "        clean_data = data[[target_col, predictor]].dropna()\n",
    "        n = len(clean_data)\n",
    "        \n",
    "        if n < max_lag + 10:\n",
    "            print(f\"⚠️  Insufficient data (n={n}). Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Method 1: Cross-Correlation Function (Most reliable for differenced data)\n",
    "        print(\"\\n--- Cross-Correlation Analysis ---\")\n",
    "        ccf_values = ccf(clean_data[predictor], clean_data[target_col], adjusted=False)[:max_lag+1]\n",
    "        \n",
    "        # Significance threshold: ±1.96/sqrt(n) for 95% confidence\n",
    "        threshold = 1.96 / np.sqrt(n)\n",
    "        \n",
    "        significant_lags_ccf = []\n",
    "        for lag in range(max_lag + 1):\n",
    "            if abs(ccf_values[lag]) > threshold:\n",
    "                significant_lags_ccf.append(lag)\n",
    "                sig_marker = '***' if abs(ccf_values[lag]) > 2*threshold else '**'\n",
    "                print(f\"  Lag {lag}: {ccf_values[lag]:.4f} {sig_marker}\")\n",
    "        \n",
    "        if not significant_lags_ccf:\n",
    "            print(\"  No significant lags found\")\n",
    "        \n",
    "        # Method 2: Individual Lag Regressions (More powerful than Granger for single predictors)\n",
    "        print(\"\\n--- Individual Lag Regression t-tests ---\")\n",
    "        from statsmodels.api import OLS, add_constant\n",
    "        \n",
    "        significant_lags_regression = []\n",
    "        regression_results = {}\n",
    "        \n",
    "        for lag in range(1, max_lag + 1):\n",
    "            # Create lagged predictor\n",
    "            lagged_data = pd.DataFrame({\n",
    "                'y': clean_data[target_col].values[lag:],\n",
    "                'x_lag': clean_data[predictor].values[:-lag]\n",
    "            })\n",
    "            \n",
    "            X = add_constant(lagged_data['x_lag'])\n",
    "            y = lagged_data['y']\n",
    "            \n",
    "            model = OLS(y, X).fit()\n",
    "            p_value = model.pvalues['x_lag']\n",
    "            coef = model.params['x_lag']\n",
    "            t_stat = model.tvalues['x_lag']\n",
    "            regression_results[lag] = {'coef': coef, 'p_value': p_value, 't_stat': t_stat}\n",
    "            \n",
    "            if p_value < 0.1:  # Using 10% threshold for screening\n",
    "                significant_lags_regression.append(lag)\n",
    "                sig_marker = '***' if p_value < 0.01 else '**' if p_value < 0.05 else '*'\n",
    "                print(f\"  Lag {lag}: coef = {coef:.6f}, t-stat = {t_stat:.3f}, p = {p_value:.4f} {sig_marker}\")\n",
    "        \n",
    "        if not significant_lags_regression:\n",
    "            print(\"  No significant lags found (p < 0.1)\")\n",
    "        \n",
    "        # Method 3: Stepwise regression (test multiple lags together)\n",
    "        print(\"\\n--- Stepwise Regression (controlling for other lags) ---\")\n",
    "        from statsmodels.api import OLS, add_constant\n",
    "        \n",
    "        # Test all lags together\n",
    "        lag_data = pd.DataFrame({'y': clean_data[target_col].values[max_lag:]})\n",
    "        \n",
    "        for lag in range(1, max_lag + 1):\n",
    "            lag_data[f'x_lag{lag}'] = clean_data[predictor].values[max_lag-lag:-lag]\n",
    "        \n",
    "        X_full = add_constant(lag_data.drop('y', axis=1))\n",
    "        y_full = lag_data['y']\n",
    "        \n",
    "        model_full = OLS(y_full, X_full).fit()\n",
    "        \n",
    "        print(f\"\\n  Full model R² = {model_full.rsquared:.4f}, Adj. R² = {model_full.rsquared_adj:.4f}\")\n",
    "        print(\"  Significant lags in joint model:\")\n",
    "        \n",
    "        joint_significant = []\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            col_name = f'x_lag{lag}'\n",
    "            if col_name in model_full.pvalues.index:\n",
    "                p_val = model_full.pvalues[col_name]\n",
    "                if p_val < 0.1:\n",
    "                    joint_significant.append(lag)\n",
    "                    coef = model_full.params[col_name]\n",
    "                    sig_marker = '***' if p_val < 0.01 else '**' if p_val < 0.05 else '*'\n",
    "                    print(f\"    Lag {lag}: coef = {coef:.6f}, p = {p_val:.4f} {sig_marker}\")\n",
    "        \n",
    "        if not joint_significant:\n",
    "            print(\"    No significant lags in joint model\")\n",
    "        \n",
    "        # Store results\n",
    "        results[predictor] = {\n",
    "            'ccf_values': ccf_values,\n",
    "            'ccf_threshold': threshold,\n",
    "            'significant_lags_ccf': significant_lags_ccf,\n",
    "            'significant_lags_regression': significant_lags_regression,\n",
    "            'regression_results': regression_results,\n",
    "            'joint_significant': joint_significant,\n",
    "            'full_model_r2': model_full.rsquared_adj,\n",
    "            'n_obs': n\n",
    "        }\n",
    "        \n",
    "        # Summary recommendation\n",
    "        print(\"\\n--- RECOMMENDATION ---\")\n",
    "        # Combine evidence from different methods\n",
    "        all_significant = set(significant_lags_ccf) | set(significant_lags_regression) | set(joint_significant)\n",
    "        \n",
    "        if all_significant:\n",
    "            recommended_lags = sorted(list(all_significant))\n",
    "            print(f\"  ✓ Recommended lags to include: {recommended_lags}\")\n",
    "            \n",
    "            # Categorize by strength of evidence\n",
    "            strong_lags = set(significant_lags_ccf) & set(significant_lags_regression)\n",
    "            if strong_lags:\n",
    "                print(f\"  ✓✓ Strongest evidence for lags: {sorted(list(strong_lags))}\")\n",
    "        else:\n",
    "            print(f\"  ✗ No significant lags found. Consider excluding this predictor or using contemporaneous value only.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_lag_analysis(results, data, target_col='real_prices_diff'):\n",
    "    \"\"\"\n",
    "    Create visualizations of lag relationships\n",
    "    \"\"\"\n",
    "    predictors = list(results.keys())\n",
    "    n_predictors = len(predictors)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_predictors, 2, figsize=(14, 4*n_predictors))\n",
    "    if n_predictors == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, predictor in enumerate(predictors):\n",
    "        res = results[predictor]\n",
    "        max_lag = len(res['ccf_values']) - 1\n",
    "        lags = range(max_lag + 1)\n",
    "        \n",
    "        # Plot 1: Cross-correlation\n",
    "        ax1 = axes[idx, 0]\n",
    "        ax1.stem(lags, res['ccf_values'], basefmt=' ')\n",
    "        ax1.axhline(y=res['ccf_threshold'], color='r', linestyle='--', label='95% CI')\n",
    "        ax1.axhline(y=-res['ccf_threshold'], color='r', linestyle='--')\n",
    "        ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "        ax1.set_xlabel('Lag')\n",
    "        ax1.set_ylabel('Cross-correlation')\n",
    "        ax1.set_title(f'{predictor}\\nCross-correlation with {target_col}')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight significant lags\n",
    "        for lag in res['significant_lags_ccf']:\n",
    "            ax1.axvspan(lag-0.3, lag+0.3, alpha=0.2, color='green')\n",
    "        \n",
    "        # Plot 2: Regression coefficients and p-values\n",
    "        ax2 = axes[idx, 1]\n",
    "        reg_results = res['regression_results']\n",
    "        lags_reg = list(reg_results.keys())\n",
    "        coefs = [reg_results[lag]['coef'] for lag in lags_reg]\n",
    "        p_values = [reg_results[lag]['p_value'] for lag in lags_reg]\n",
    "        \n",
    "        # Bar plot of coefficients with color based on significance\n",
    "        colors = ['green' if p < 0.05 else 'orange' if p < 0.1 else 'gray' \n",
    "                  for p in p_values]\n",
    "        ax2.bar(lags_reg, coefs, color=colors, alpha=0.6)\n",
    "        ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "        ax2.set_xlabel('Lag')\n",
    "        ax2.set_ylabel('Regression Coefficient')\n",
    "        ax2.set_title(f'{predictor}\\nIndividual Lag Coefficients\\n(Green: p<0.05, Orange: p<0.1)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_optimal_lag_model(data, results, target_col='real_prices_diff', \n",
    "                             min_lag_evidence=1):\n",
    "    \"\"\"\n",
    "    Create a regression model using the identified significant lags\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_lag_evidence : int\n",
    "        Minimum number of methods that must agree on lag significance (1-3)\n",
    "    \"\"\"\n",
    "    from statsmodels.api import OLS, add_constant\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BUILDING OPTIMAL LAG MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Collect all recommended lags\n",
    "    model_features = {}\n",
    "    \n",
    "    for predictor, res in results.items():\n",
    "        # Count evidence for each lag\n",
    "        all_lags = set(res['significant_lags_ccf']) | \\\n",
    "                   set(res['significant_lags_regression']) | \\\n",
    "                   set(res.get('joint_significant', []))\n",
    "        \n",
    "        if all_lags:\n",
    "            # For each lag, count how many methods found it significant\n",
    "            lag_evidence = {}\n",
    "            for lag in all_lags:\n",
    "                evidence = 0\n",
    "                if lag in res['significant_lags_ccf']:\n",
    "                    evidence += 1\n",
    "                if lag in res['significant_lags_regression']:\n",
    "                    evidence += 1\n",
    "                if lag in res.get('joint_significant', []):\n",
    "                    evidence += 1\n",
    "                lag_evidence[lag] = evidence\n",
    "            \n",
    "            # Keep lags with sufficient evidence\n",
    "            selected_lags = [lag for lag, ev in lag_evidence.items() \n",
    "                           if ev >= min_lag_evidence]\n",
    "            \n",
    "            if selected_lags:\n",
    "                model_features[predictor] = sorted(selected_lags)\n",
    "                print(f\"\\n{predictor}:\")\n",
    "                print(f\"  Selected lags: {selected_lags}\")\n",
    "    \n",
    "    # Build the model\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Building regression model...\")\n",
    "    \n",
    "    # Determine maximum lag needed\n",
    "    max_lag_needed = max([max(lags) for lags in model_features.values()])\n",
    "    \n",
    "    # Create lagged features\n",
    "    model_data = pd.DataFrame({target_col: data[target_col].values[max_lag_needed:]})\n",
    "    \n",
    "    for predictor, lags in model_features.items():\n",
    "        for lag in lags:\n",
    "            col_name = f'{predictor}_lag{lag}'\n",
    "            model_data[col_name] = data[predictor].values[max_lag_needed-lag:-lag]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    model_data = model_data.dropna()\n",
    "    \n",
    "    print(f\"\\nModel data shape: {model_data.shape}\")\n",
    "    print(f\"Number of features: {model_data.shape[1] - 1}\")\n",
    "    print(f\"Number of observations: {model_data.shape[0]}\")\n",
    "    \n",
    "    # Fit model\n",
    "    y = model_data[target_col]\n",
    "    X = add_constant(model_data.drop(target_col, axis=1))\n",
    "    \n",
    "    model = OLS(y, X).fit()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model, model_data, model_features\n",
    "\n",
    "\n",
    "def forecast_one_step(model, data, model_features, target_col='real_prices_diff'):\n",
    "    \"\"\"\n",
    "    Make a one-step ahead forecast using the fitted model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : statsmodels regression results\n",
    "        The fitted OLS model\n",
    "    data : pd.DataFrame\n",
    "        The original data with all variables\n",
    "    model_features : dict\n",
    "        Dictionary with predictor names as keys and lists of lags as values\n",
    "    target_col : str\n",
    "        Name of the target variable\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    forecast : float\n",
    "        One-step ahead forecast\n",
    "    forecast_df : pd.DataFrame\n",
    "        DataFrame with the feature values used for prediction\n",
    "    \"\"\"\n",
    "    from statsmodels.api import add_constant\n",
    "    \n",
    "    # Get the most recent values needed for prediction\n",
    "    max_lag_needed = max([max(lags) for lags in model_features.values()])\n",
    "    \n",
    "    # Create the feature vector for prediction\n",
    "    forecast_features = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ONE-STEP AHEAD FORECAST\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nUsing most recent {max_lag_needed} observations for lagged features:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for predictor, lags in model_features.items():\n",
    "        print(f\"\\n{predictor}:\")\n",
    "        for lag in lags:\n",
    "            # Get the value from 'lag' periods ago\n",
    "            feature_name = f'{predictor}_lag{lag}'\n",
    "            feature_value = data[predictor].iloc[-lag]\n",
    "            forecast_features[feature_name] = feature_value\n",
    "            \n",
    "            # Show which historical value is being used\n",
    "            date_used = data.index[-lag] if hasattr(data.index[-lag], 'strftime') else f\"observation t-{lag}\"\n",
    "            print(f\"  {feature_name}: {feature_value:.6f} (from {date_used})\")\n",
    "    \n",
    "    # Create DataFrame for prediction\n",
    "    forecast_df = pd.DataFrame([forecast_features])\n",
    "    \n",
    "    # Add constant term\n",
    "    forecast_df_with_const = add_constant(forecast_df, has_constant='add')\n",
    "    \n",
    "    # Make sure columns match the model\n",
    "    model_cols = [col for col in model.model.exog_names if col in forecast_df_with_const.columns or col == 'const']\n",
    "    forecast_df_with_const = forecast_df_with_const[model_cols]\n",
    "    \n",
    "    # Generate forecast\n",
    "    forecast = model.predict(forecast_df_with_const)[0]\n",
    "    \n",
    "    # Get prediction interval\n",
    "    prediction = model.get_prediction(forecast_df_with_const)\n",
    "    pred_summary = prediction.summary_frame(alpha=0.05)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FORECAST RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nOne-step ahead forecast: {forecast:.6f}\")\n",
    "    print(f\"95% Prediction Interval: [{pred_summary['obs_ci_lower'].values[0]:.6f}, {pred_summary['obs_ci_upper'].values[0]:.6f}]\")\n",
    "    print(f\"Standard Error: {pred_summary['mean_se'].values[0]:.6f}\")\n",
    "    \n",
    "    return forecast, forecast_df, pred_summary\n",
    "\n",
    "\n",
    "def rolling_forecast_validation(data, model_features, target_col='real_prices_diff', \n",
    "                                test_size=0.2, expanding_window=True):\n",
    "    \"\"\"\n",
    "    Perform rolling one-step ahead forecasts for model validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The full dataset\n",
    "    model_features : dict\n",
    "        Dictionary with predictor names as keys and lists of lags as values\n",
    "    target_col : str\n",
    "        Name of the target variable\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing (default 0.2 = 20%)\n",
    "    expanding_window : bool\n",
    "        If True, use expanding window (training set grows each iteration)\n",
    "        If False, use rolling window (training set size stays constant)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    forecast_results : pd.DataFrame\n",
    "        DataFrame with actual values, forecasts, and errors\n",
    "    \"\"\"\n",
    "    from statsmodels.api import OLS, add_constant\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ROLLING ONE-STEP AHEAD FORECAST VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    max_lag_needed = max([max(lags) for lags in model_features.values()])\n",
    "    \n",
    "    # Initialize results storage\n",
    "    forecasts = []\n",
    "    actuals = []\n",
    "    dates = []\n",
    "    \n",
    "    # Calculate train/test split\n",
    "    total_obs = len(data)\n",
    "    n_forecast = int(total_obs * test_size)\n",
    "    min_train_size = total_obs - n_forecast\n",
    "    \n",
    "    window_type = \"expanding\" if expanding_window else \"rolling\"\n",
    "    \n",
    "    print(f\"\\nTotal observations: {total_obs}\")\n",
    "    print(f\"Training set (initial): {min_train_size} observations ({(1-test_size)*100:.0f}%)\")\n",
    "    print(f\"Test set: {n_forecast} observations ({test_size*100:.0f}%)\")\n",
    "    print(f\"Window type: {window_type}\")\n",
    "    print(f\"Max lag used: {max_lag_needed}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    print(f\"\\nGenerating {n_forecast} one-step ahead forecasts...\")\n",
    "    print(f\"Window type: {window_type}\")\n",
    "    \n",
    "    for i in range(n_forecast):\n",
    "        if expanding_window:\n",
    "            # Expanding window: training set grows\n",
    "            train_start = 0\n",
    "            train_end = min_train_size + i\n",
    "        else:\n",
    "            # Rolling window: training set size stays constant\n",
    "            train_start = i\n",
    "            train_end = min_train_size + i\n",
    "        \n",
    "        # Create training data\n",
    "        train_data = data.iloc[train_start:train_end]\n",
    "        \n",
    "        # Build model on training data\n",
    "        model_train_data = pd.DataFrame({target_col: train_data[target_col].values[max_lag_needed:]})\n",
    "        \n",
    "        for predictor, lags in model_features.items():\n",
    "            for lag in lags:\n",
    "                col_name = f'{predictor}_lag{lag}'\n",
    "                model_train_data[col_name] = train_data[predictor].values[max_lag_needed-lag:-lag]\n",
    "        \n",
    "        model_train_data = model_train_data.dropna()\n",
    "        \n",
    "        # Fit model\n",
    "        y_train = model_train_data[target_col]\n",
    "        X_train = add_constant(model_train_data.drop(target_col, axis=1))\n",
    "        model = OLS(y_train, X_train).fit()\n",
    "        \n",
    "        # Make one-step ahead forecast\n",
    "        forecast_features = {}\n",
    "        for predictor, lags in model_features.items():\n",
    "            for lag in lags:\n",
    "                feature_name = f'{predictor}_lag{lag}'\n",
    "                forecast_features[feature_name] = train_data[predictor].iloc[-lag]\n",
    "        \n",
    "        forecast_df = pd.DataFrame([forecast_features])\n",
    "        forecast_df_with_const = add_constant(forecast_df, has_constant='add')\n",
    "        \n",
    "        # Ensure column order matches\n",
    "        forecast_df_with_const = forecast_df_with_const[model.model.exog_names]\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = model.predict(forecast_df_with_const)[0]\n",
    "        \n",
    "        # Get actual value\n",
    "        actual = data[target_col].iloc[train_end]\n",
    "        \n",
    "        forecasts.append(forecast)\n",
    "        actuals.append(actual)\n",
    "        dates.append(data.index[train_end])\n",
    "        \n",
    "        if (i + 1) % max(1, n_forecast // 10) == 0 or i == 0 or i == n_forecast - 1:\n",
    "            print(f\"Forecast {i+1}/{n_forecast}: Train size={len(y_train)}, Predicted={forecast:.6f}, Actual={actual:.6f}, Error={actual-forecast:.6f}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    forecast_results = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'actual': actuals,\n",
    "        'forecast': forecasts,\n",
    "        'error': np.array(actuals) - np.array(forecasts)\n",
    "    })\n",
    "    \n",
    "    forecast_results['abs_error'] = forecast_results['error'].abs()\n",
    "    forecast_results['squared_error'] = forecast_results['error'] ** 2\n",
    "    forecast_results['abs_pct_error'] = (forecast_results['abs_error'] / forecast_results['actual'].abs()) * 100\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FORECAST ACCURACY METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nMean Error (ME): {forecast_results['error'].mean():.6f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {forecast_results['abs_error'].mean():.6f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {np.sqrt(forecast_results['squared_error'].mean()):.6f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {forecast_results['abs_pct_error'].mean():.2f}%\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(forecast_results['date'], forecast_results['actual'], 'o-', label='Actual', linewidth=2)\n",
    "    plt.plot(forecast_results['date'], forecast_results['forecast'], 's--', label='Forecast', linewidth=2)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.title('Rolling One-Step Ahead Forecasts vs Actual Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot forecast errors\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(range(len(forecast_results)), forecast_results['error'], alpha=0.6)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    plt.xlabel('Forecast Number')\n",
    "    plt.ylabel('Forecast Error')\n",
    "    plt.title('One-Step Ahead Forecast Errors')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return forecast_results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# 1. Find significant lags\n",
    "results = find_significant_lags(data, max_lag=12)\n",
    "plot_lag_analysis(results, data)\n",
    "\n",
    "# 2. Build model on full data (for exploration)\n",
    "model, model_data, features = create_optimal_lag_model(data, results, min_lag_evidence=1)\n",
    "\n",
    "# 3. Make a single one-step ahead forecast\n",
    "forecast, forecast_df, pred_summary = forecast_one_step(model, data, features)\n",
    "\n",
    "# 4. Validate with rolling forecasts on last 20% of data\n",
    "forecast_results = rolling_forecast_validation(data, features, test_size=0.2, expanding_window=True)\n",
    "\n",
    "# Alternative: Use rolling window instead of expanding\n",
    "#forecast_results = rolling_forecast_validation(data, features, test_size=0.2, expanding_window=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72774570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Apply second differencing\n",
    "loans_diff_2 = data['loans_diff'].diff().dropna()\n",
    "share_net_worth_bottom_50_diff_2 = data['share_net_worth_bottom_50_diff'].diff().dropna()\n",
    "\n",
    "# ADF Test for second differenced series\n",
    "adf_result_loans_2 = adfuller(loans_diff_2)\n",
    "adf_result_share_net_worth_2 = adfuller(share_net_worth_bottom_50_diff_2)\n",
    "\n",
    "print(f\"ADF Statistic for loans_diff_2: {adf_result_loans_2[0]}\")\n",
    "print(f\"p-value for loans_diff_2: {adf_result_loans_2[1]}\")\n",
    "\n",
    "print(f\"ADF Statistic for share_net_worth_bottom_50_diff_2: {adf_result_share_net_worth_2[0]}\")\n",
    "print(f\"p-value for share_net_worth_bottom_50_diff_2: {adf_result_share_net_worth_2[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PCA and Regression Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = data[[\n",
    "'loans_diff',\n",
    "'net_wealth_diff',\n",
    "'vacancy_rate_diff',\n",
    "'mortgage_rate_diff',\n",
    "# 'active_listings_diff',\n",
    "'median_house_income_diff',\n",
    "'monthly_supply_homes_diff',\n",
    "'labor_share_diff',\n",
    "'unemployment_rate_diff',\n",
    "'share_net_worth_bottom_50_diff' ]]\n",
    "y = data['real_prices_diff']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: STANDARDIZATION\")\n",
    "print(\"=\" * 60)\n",
    "# Standardize features (required for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeatures standardized (mean=0, std=1)\")\n",
    "print(\"Training set shape:\", X_train_scaled.shape)\n",
    "print(\"Test set shape:\", X_test_scaled.shape)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: PCA FACTOR ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Display explained variance\n",
    "print(\"\\nExplained variance by each component:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_, 1):\n",
    "    print(f\"  PC{i}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative explained variance:\")\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "for i, var in enumerate(cumsum, 1):\n",
    "    print(f\"  PC1-PC{i}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "# Component loadings\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i}' for i in range(1, len(pca.components_) + 1)],\n",
    "    index=X.columns\n",
    ")\n",
    "print(\"\\nPCA Loadings (Component Matrix):\")\n",
    "print(loadings.round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: DETERMINE NUMBER OF FACTORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Kaiser criterion: eigenvalues > 1\n",
    "eigenvalues = pca.explained_variance_\n",
    "print(\"\\nEigenvalues:\")\n",
    "for i, ev in enumerate(eigenvalues, 1):\n",
    "    print(f\"  PC{i}: {ev:.4f}\", \"✓ (Keep)\" if ev > 1 else \"  (Drop)\")\n",
    "\n",
    "n_components_kaiser = sum(eigenvalues > 1)\n",
    "print(f\"\\nKaiser criterion suggests keeping {n_components_kaiser} component(s)\")\n",
    "\n",
    "# Choose based on variance threshold (e.g., 80%)\n",
    "variance_threshold = 0.80\n",
    "n_components_var = np.argmax(cumsum >= variance_threshold) + 1\n",
    "print(f\"To explain {variance_threshold*100}% variance: {n_components_var} component(s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: REGRESSION WITH PRINCIPAL COMPONENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the number of components that explain ~80-90% variance\n",
    "n_final = max(n_components_kaiser, n_components_var)\n",
    "print(f\"\\nUsing {n_final} principal component(s) for regression\")\n",
    "\n",
    "X_train_pca_reduced = X_train_pca[:, :n_final]\n",
    "X_test_pca_reduced = X_test_pca[:, :n_final]\n",
    "\n",
    "# Fit regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_pca_reduced, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train_pca_reduced)\n",
    "y_test_pred = model.predict(X_test_pca_reduced)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  Training R²: {train_r2:.4f}\")\n",
    "print(f\"  Test R²: {test_r2:.4f}\")\n",
    "print(f\"  Training RMSE: {train_rmse:.6f}\")\n",
    "print(f\"  Test RMSE: {test_rmse:.6f}\")\n",
    "print(f\"  Training MAE: {train_mae:.6f}\")\n",
    "print(f\"  Test MAE: {test_mae:.6f}\")\n",
    "\n",
    "print(\"\\nRegression Coefficients (for PCs):\")\n",
    "for i, coef in enumerate(model.coef_, 1):\n",
    "    print(f\"  PC{i}: {coef:.4f}\")\n",
    "print(f\"  Intercept: {model.intercept_:.6f}\")\n",
    "\n",
    "# Interpretation: Map PC coefficients back to original variables\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION: Effect on Price Changes\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nApproximate contribution of original variables:\")\n",
    "print(\"(PC coefficients × loadings)\")\n",
    "\n",
    "original_effects = np.dot(loadings.iloc[:, :n_final], model.coef_)\n",
    "for var, effect in zip(X.columns, original_effects):\n",
    "    print(f\"  {var}: {effect:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Scree plot\n",
    "axes[0, 0].bar(range(1, len(eigenvalues) + 1), eigenvalues, alpha=0.7, color='steelblue')\n",
    "axes[0, 0].axhline(y=1, color='r', linestyle='--', label='Kaiser criterion', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Principal Component', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Eigenvalue', fontsize=11)\n",
    "axes[0, 0].set_title('Scree Plot', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Cumulative variance explained\n",
    "axes[0, 1].plot(range(1, len(cumsum) + 1), cumsum, marker='o', linewidth=2, \n",
    "                color='steelblue', markersize=8)\n",
    "axes[0, 1].axhline(y=0.8, color='r', linestyle='--', label='80% variance', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Number of Components', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Cumulative Explained Variance', fontsize=11)\n",
    "axes[0, 1].set_title('Cumulative Variance Explained', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylim([0, 1.05])\n",
    "\n",
    "# 3. Component loadings heatmap\n",
    "sns.heatmap(loadings.iloc[:, :n_final], annot=True, cmap='RdBu_r', center=0, \n",
    "            ax=axes[1, 0], cbar_kws={'label': 'Loading'}, fmt='.3f',\n",
    "            vmin=-1, vmax=1, linewidths=0.5)\n",
    "axes[1, 0].set_title('PCA Loadings Heatmap', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Principal Component', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Original Variable (Differenced)', fontsize=11)\n",
    "\n",
    "# 4. Actual vs Predicted\n",
    "axes[1, 1].scatter(y_test, y_test_pred, alpha=0.6, color='steelblue', \n",
    "                   edgecolors='black', s=50)\n",
    "min_val = min(y_test.min(), y_test_pred.min())\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, \n",
    "                label='Perfect prediction')\n",
    "axes[1, 1].set_xlabel('Actual Price Change (Log Diff)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Predicted Price Change (Log Diff)', fontsize=11)\n",
    "axes[1, 1].set_title(f'Actual vs Predicted (Test Set)\\nR² = {test_r2:.4f}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_analysis_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualizations saved as 'pca_analysis_results.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNote: You're modeling CHANGES in log prices using CHANGES in predictors.\")\n",
    "print(\"This is appropriate for non-stationary time series data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311df00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========================================\n",
    "# 1. Baseline: AR(2) Model\n",
    "# ========================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE: AR(2) MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data for AR model\n",
    "y_series = data['real_prices_diff'].dropna()\n",
    "\n",
    "# Split data (80-20, time-ordered)\n",
    "train_size = int(len(y_series) * 0.8)\n",
    "y_train_ar = y_series.iloc[:train_size]\n",
    "y_test_ar = y_series.iloc[train_size:]\n",
    "\n",
    "# Fit AR(2) model\n",
    "ar2_model = AutoReg(y_train_ar, lags=2, trend='n').fit()\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_ar2 = ar2_model.predict(start=len(y_train_ar), end=len(y_series)-1)\n",
    "\n",
    "# Metrics\n",
    "ar2_r2 = r2_score(y_test_ar, y_test_pred_ar2)\n",
    "ar2_rmse = np.sqrt(mean_squared_error(y_test_ar, y_test_pred_ar2))\n",
    "ar2_mae = mean_absolute_error(y_test_ar, y_test_pred_ar2)\n",
    "\n",
    "print(f\"\\nAR(2) Performance:\")\n",
    "print(f\"  Test R²: {ar2_r2:.4f}\")\n",
    "print(f\"  Test RMSE: {ar2_rmse:.4f}\")\n",
    "print(f\"  Test MAE: {ar2_mae:.4f}\")\n",
    "print(f\"\\n→ This is the benchmark to beat!\")\n",
    "\n",
    "# ========================================\n",
    "# 2. Advanced Feature Engineering\n",
    "# ========================================\n",
    "\n",
    "def create_advanced_features(data, max_lag=6):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set including:\n",
    "    - Lagged features\n",
    "    - Autoregressive terms (AR)\n",
    "    - Rolling statistics\n",
    "    - Interaction terms\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in df.columns if col != 'real_prices_diff']\n",
    "    \n",
    "    # === 1. Autoregressive terms (critical for beating AR(2)) ===\n",
    "    df['price_lag1'] = df['real_prices_diff'].shift(1)\n",
    "    df['price_lag2'] = df['real_prices_diff'].shift(2)\n",
    "    df['price_lag3'] = df['real_prices_diff'].shift(3)\n",
    "    \n",
    "    # === 2. Rolling statistics of price ===\n",
    "    df['price_rolling_mean_3'] = df['real_prices_diff'].shift(1).rolling(3).mean()\n",
    "    df['price_rolling_std_3'] = df['real_prices_diff'].shift(1).rolling(3).std()\n",
    "    df['price_rolling_mean_6'] = df['real_prices_diff'].shift(1).rolling(6).mean()\n",
    "    \n",
    "    # === 3. Lagged exogenous variables ===\n",
    "    for col in feature_cols:\n",
    "        df[f'{col}_lag1'] = df[col].shift(1)\n",
    "        df[f'{col}_lag3'] = df[col].shift(3)\n",
    "        df[f'{col}_lag6'] = df[col].shift(6)\n",
    "        \n",
    "        # Rolling averages of key variables\n",
    "        if col in ['loans_diff', 'mortgage_rate_diff', 'unemployment_rate_diff']:\n",
    "            df[f'{col}_rolling_mean_3'] = df[col].shift(1).rolling(3).mean()\n",
    "    \n",
    "    # === 4. Interaction terms (key relationships) ===\n",
    "    # Mortgage rate * income (affordability)\n",
    "    df['mortgage_income_interact'] = (\n",
    "        df['mortgage_rate_diff_lag1'] * df['median_house_income_diff_lag1']\n",
    "    )\n",
    "    \n",
    "    # Supply * vacancy (market tightness)\n",
    "    df['supply_vacancy_interact'] = (\n",
    "        df['monthly_supply_homes_diff_lag1'] * df['vacancy_rate_diff_lag1']\n",
    "    )\n",
    "    \n",
    "    # Loans * wealth (credit conditions)\n",
    "    df['loans_wealth_interact'] = (\n",
    "        df['loans_diff_lag1'] * df['net_wealth_diff_lag1']\n",
    "    )\n",
    "    \n",
    "    # === 5. Momentum indicators ===\n",
    "    # Price momentum (change in change)\n",
    "    df['price_momentum'] = df['price_lag1'] - df['price_lag2']\n",
    "    \n",
    "    # === 6. Current period features ===\n",
    "    for col in feature_cols:\n",
    "        df[f'{col}_current'] = df[col]\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING ADVANCED FEATURES...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data_advanced = create_advanced_features(data, max_lag=6)\n",
    "\n",
    "print(f\"\\nOriginal features: {data.shape[1]}\")\n",
    "print(f\"Advanced features: {data_advanced.shape[1]}\")\n",
    "print(f\"Rows after lagging: {len(data_advanced)}\")\n",
    "\n",
    "# ========================================\n",
    "# 3. Test Multiple ML Models\n",
    "# ========================================\n",
    "\n",
    "# Prepare data\n",
    "X = data_advanced.drop('real_prices_diff', axis=1)\n",
    "y = data_advanced['real_prices_diff']\n",
    "\n",
    "# Time-ordered split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Ridge (L2)': Ridge(alpha=1.0),\n",
    "    'Lasso (L1)': Lasso(alpha=0.001, max_iter=10000)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING MODELS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Check if beats AR(2)\n",
    "    beats_ar2 = \"✓\" if test_r2 > ar2_r2 else \"✗\"\n",
    "    improvement = test_r2 - ar2_r2\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train R²': train_r2,\n",
    "        'Test R²': test_r2,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Test MAE': test_mae,\n",
    "        'Overfit': train_r2 - test_r2,\n",
    "        'Beats AR(2)': beats_ar2,\n",
    "        'Improvement': improvement\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test R²: {test_r2:.4f} {beats_ar2}\")\n",
    "    print(f\"  Improvement over AR(2): {improvement:+.4f}\")\n",
    "\n",
    "# Add AR(2) to results for comparison\n",
    "results.insert(0, {\n",
    "    'Model': 'AR(2) Baseline',\n",
    "    'Train R²': np.nan,\n",
    "    'Test R²': ar2_r2,\n",
    "    'Test RMSE': ar2_rmse,\n",
    "    'Test MAE': ar2_mae,\n",
    "    'Overfit': np.nan,\n",
    "    'Beats AR(2)': '—',\n",
    "    'Improvement': 0.0\n",
    "})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# ========================================\n",
    "# 4. Best Model Analysis\n",
    "# ========================================\n",
    "\n",
    "# Find best model (excluding AR(2))\n",
    "best_idx = results_df[results_df['Model'] != 'AR(2) Baseline']['Test R²'].idxmax()\n",
    "best_model_name = results_df.loc[best_idx, 'Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "y_test_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features:\")\n",
    "    print(feature_importance.head(15).to_string(index=False))\n",
    "    \n",
    "    # Check if AR terms are important\n",
    "    ar_features = feature_importance[feature_importance['feature'].str.contains('price_lag')]\n",
    "    print(\"\\n→ Autoregressive Feature Importance:\")\n",
    "    print(ar_features.to_string(index=False))\n",
    "\n",
    "# ========================================\n",
    "# 5. Visualizations\n",
    "# ========================================\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Model Comparison\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "models_sorted = results_df.sort_values('Test R²', ascending=True)\n",
    "colors = ['red' if m == 'AR(2) Baseline' else 'steelblue' for m in models_sorted['Model']]\n",
    "bars = ax1.barh(models_sorted['Model'], models_sorted['Test R²'], color=colors, edgecolor='black')\n",
    "ax1.axvline(x=ar2_r2, color='red', linestyle='--', linewidth=2, label=f'AR(2) Baseline (R²={ar2_r2:.3f})')\n",
    "ax1.set_xlabel('Test R²', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, models_sorted['Test R²'])):\n",
    "    ax1.text(val, bar.get_y() + bar.get_height()/2, f'  {val:.4f}',\n",
    "             va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Actual vs Predicted (Best Model)\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.scatter(y_test, y_test_pred_best, alpha=0.6, edgecolors='k', linewidth=0.5, s=50)\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('Actual Price Change', fontsize=11)\n",
    "ax2.set_ylabel('Predicted Price Change', fontsize=11)\n",
    "ax2.set_title(f'{best_model_name}: Actual vs Predicted\\nR² = {r2_score(y_test, y_test_pred_best):.4f}', \n",
    "              fontsize=11, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals (Best Model)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "residuals = y_test - y_test_pred_best\n",
    "ax3.scatter(y_test_pred_best, residuals, alpha=0.6, edgecolors='k', linewidth=0.5, s=50)\n",
    "ax3.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax3.set_xlabel('Predicted Price Change', fontsize=11)\n",
    "ax3.set_ylabel('Residuals', fontsize=11)\n",
    "ax3.set_title(f'{best_model_name}: Residual Plot', fontsize=11, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature Importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    top_features = feature_importance.head(15)\n",
    "    \n",
    "    # Color code: AR features vs exogenous\n",
    "    colors_feat = ['darkred' if 'price_lag' in f or 'price_rolling' in f or 'price_momentum' in f\n",
    "                   else 'steelblue' for f in top_features['feature']]\n",
    "    \n",
    "    ax4.barh(range(len(top_features)), top_features['importance'], color=colors_feat, edgecolor='black')\n",
    "    ax4.set_yticks(range(len(top_features)))\n",
    "    ax4.set_yticklabels(top_features['feature'], fontsize=8)\n",
    "    ax4.set_xlabel('Importance', fontsize=11)\n",
    "    ax4.set_title(f'Top 15 Features\\n(Red = AR features)', fontsize=11, fontweight='bold')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========================================\n",
    "# 6. Interpretation & Recommendations\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION & NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_r2 = results_df.loc[best_idx, 'Test R²']\n",
    "improvement = best_r2 - ar2_r2\n",
    "\n",
    "print(f\"\\n1. DID WE BEAT AR(2)?\")\n",
    "if improvement > 0.01:\n",
    "    print(f\"   ✓ YES! Improvement of {improvement:.4f} ({improvement/abs(ar2_r2)*100:.1f}%)\")\n",
    "    print(f\"   → {best_model_name} with exogenous features beats pure autoregression\")\n",
    "elif improvement > 0:\n",
    "    print(f\"   ⚠ MARGINAL. Improvement of only {improvement:.4f}\")\n",
    "    print(f\"   → External features add minimal value beyond AR(2)\")\n",
    "else:\n",
    "    print(f\"   ✗ NO. Performance is {improvement:.4f} worse than AR(2)\")\n",
    "    print(f\"   → Price changes are mainly driven by their own history\")\n",
    "\n",
    "print(f\"\\n2. KEY INSIGHTS:\")\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    ar_total_importance = ar_features['importance'].sum()\n",
    "    print(f\"   - Autoregressive features account for {ar_total_importance:.1%} of importance\")\n",
    "    \n",
    "    exog_important = feature_importance[~feature_importance['feature'].str.contains('price_')]\n",
    "    if len(exog_important) > 0:\n",
    "        top_exog = exog_important.iloc[0]\n",
    "        print(f\"   - Most important external factor: {top_exog['feature']} ({top_exog['importance']:.3f})\")\n",
    "\n",
    "print(f\"\\n3. RECOMMENDATIONS:\")\n",
    "if improvement > 0.05:\n",
    "    print(\"   → Use this ML model for forecasting - it clearly beats AR(2)\")\n",
    "    print(\"   → External economic factors provide meaningful signal\")\n",
    "elif improvement > 0:\n",
    "    print(\"   → Consider ensemble: combine AR(2) with ML predictions\")\n",
    "    print(\"   → External factors help but marginally\")\n",
    "else:\n",
    "    print(\"   → Stick with AR(2) - it's simpler and performs better\")\n",
    "    print(\"   → Try: different feature engineering, longer lags, or regime-switching models\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Advanced Machine Learning Pipeline for Time Series Forecasting\n",
    "Beats AR(2) baseline by incorporating economic features and ensemble methods\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================================================================================\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "# ================================================================================\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load your dataset - modify path as needed\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def create_advanced_features(df, target_col='price'):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set including:\n",
    "    - Autoregressive lags\n",
    "    - Rolling statistics\n",
    "    - Differences and momentum\n",
    "    - Interaction terms\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Autoregressive features (lags 1-6)\n",
    "    for lag in range(1, 7):\n",
    "        df[f'{target_col}_lag{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # 2. Rolling statistics\n",
    "    for window in [3, 6, 12]:\n",
    "        df[f'{target_col}_rolling_mean_{window}'] = df[target_col].shift(1).rolling(window).mean()\n",
    "        df[f'{target_col}_rolling_std_{window}'] = df[target_col].shift(1).rolling(window).std()\n",
    "        df[f'{target_col}_rolling_min_{window}'] = df[target_col].shift(1).rolling(window).min()\n",
    "        df[f'{target_col}_rolling_max_{window}'] = df[target_col].shift(1).rolling(window).max()\n",
    "    \n",
    "    # 3. Momentum and differences\n",
    "    df[f'{target_col}_momentum'] = df[target_col].shift(1) - df[target_col].shift(4)\n",
    "    df[f'{target_col}_diff'] = df[target_col].diff()\n",
    "    df[f'{target_col}_pct_change'] = df[target_col].pct_change()\n",
    "    \n",
    "    # 4. Exogenous variable features (adjust column names to match your data)\n",
    "    exog_cols = [col for col in df.columns if col not in ['date', target_col] and not col.endswith(tuple([f'_lag{i}' for i in range(1,7)]))]\n",
    "    \n",
    "    for col in exog_cols:\n",
    "        if df[col].dtype in ['float64', 'int64']:\n",
    "            # Current and lagged values\n",
    "            df[f'{col}_current'] = df[col]\n",
    "            for lag in [1, 2, 3]:\n",
    "                df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "            \n",
    "            # Differences\n",
    "            df[f'{col}_diff'] = df[col].diff()\n",
    "            df[f'{col}_diff_lag1'] = df[f'{col}_diff'].shift(1)\n",
    "            df[f'{col}_diff_lag3'] = df[f'{col}_diff'].shift(3)\n",
    "            \n",
    "            # Rolling statistics\n",
    "            df[f'{col}_diff_rolling_mean_3'] = df[f'{col}_diff'].shift(1).rolling(3).mean()\n",
    "    \n",
    "    # 5. Interaction terms (example - adjust based on your domain knowledge)\n",
    "    if 'loans' in exog_cols and 'wealth' in exog_cols:\n",
    "        df['loans_wealth_interact'] = df['loans'] * df['wealth']\n",
    "    \n",
    "    # 6. Seasonal features (if you have date)\n",
    "    if 'date' in df.columns:\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "        df['year'] = df['date'].dt.year\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================================================================================\n",
    "# 2. BASELINE AR(2) MODEL\n",
    "# ================================================================================\n",
    "\n",
    "def train_ar2_baseline(y_train, y_test):\n",
    "    \"\"\"Train baseline AR(2) model for comparison\"\"\"\n",
    "    ar2_model = AutoReg(y_train, lags=2).fit()\n",
    "    \n",
    "    # Forecast on test set\n",
    "    predictions = []\n",
    "    history = list(y_train)\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        lag1 = history[-1]\n",
    "        lag2 = history[-2]\n",
    "        pred = ar2_model.params[0] + ar2_model.params[1]*lag1 + ar2_model.params[2]*lag2\n",
    "        predictions.append(pred)\n",
    "        history.append(y_test.iloc[i])\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "    return {\n",
    "        'model': ar2_model,\n",
    "        'predictions': predictions,\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "# ================================================================================\n",
    "# 3. ADVANCED ML MODELS WITH TUNING\n",
    "# ================================================================================\n",
    "\n",
    "def train_random_forest_tuned(X_train, y_train, X_test, y_test, n_iter=30):\n",
    "    \"\"\"Train Random Forest with hyperparameter tuning\"\"\"\n",
    "    print(\"Training Random Forest with hyperparameter search...\")\n",
    "    \n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [10, 15, 20, 25, None],\n",
    "        'min_samples_split': [2, 5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4, 6],\n",
    "        'max_features': ['sqrt', 'log2', 0.3, 0.5]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    rf_search = RandomizedSearchCV(\n",
    "        rf, param_dist, n_iter=n_iter, \n",
    "        cv=tscv, scoring='r2', \n",
    "        n_jobs=-1, random_state=42, verbose=1\n",
    "    )\n",
    "    \n",
    "    rf_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = rf_search.best_estimator_\n",
    "    \n",
    "    train_pred = best_model.predict(X_train)\n",
    "    test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'best_params': rf_search.best_params_,\n",
    "        'train_r2': r2_score(y_train, train_pred),\n",
    "        'test_r2': r2_score(y_test, test_pred),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "        'test_mae': mean_absolute_error(y_test, test_pred),\n",
    "        'predictions': test_pred\n",
    "    }\n",
    "\n",
    "def train_gradient_boosting_tuned(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train Gradient Boosting with regularization to prevent overfitting\"\"\"\n",
    "    print(\"Training Gradient Boosting with regularization...\")\n",
    "    \n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [2, 3, 4, 5],\n",
    "        'min_samples_split': [5, 10, 20],\n",
    "        'min_samples_leaf': [2, 4, 6],\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    gb = GradientBoostingRegressor(random_state=42)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    gb_search = RandomizedSearchCV(\n",
    "        gb, param_dist, n_iter=30,\n",
    "        cv=tscv, scoring='r2',\n",
    "        n_jobs=-1, random_state=42, verbose=1\n",
    "    )\n",
    "    \n",
    "    gb_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = gb_search.best_estimator_\n",
    "    \n",
    "    train_pred = best_model.predict(X_train)\n",
    "    test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'best_params': gb_search.best_params_,\n",
    "        'train_r2': r2_score(y_train, train_pred),\n",
    "        'test_r2': r2_score(y_test, test_pred),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "        'test_mae': mean_absolute_error(y_test, test_pred),\n",
    "        'predictions': test_pred\n",
    "    }\n",
    "\n",
    "def train_ensemble(rf_model, gb_model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Create ensemble of best models\"\"\"\n",
    "    print(\"Creating ensemble model...\")\n",
    "    \n",
    "    ensemble = VotingRegressor([\n",
    "        ('rf', rf_model),\n",
    "        ('gb', gb_model)\n",
    "    ])\n",
    "    \n",
    "    ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    train_pred = ensemble.predict(X_train)\n",
    "    test_pred = ensemble.predict(X_test)\n",
    "    \n",
    "    return {\n",
    "        'model': ensemble,\n",
    "        'train_r2': r2_score(y_train, train_pred),\n",
    "        'test_r2': r2_score(y_test, test_pred),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "        'test_mae': mean_absolute_error(y_test, test_pred),\n",
    "        'predictions': test_pred\n",
    "    }\n",
    "\n",
    "# ================================================================================\n",
    "# 4. FEATURE SELECTION\n",
    "# ================================================================================\n",
    "\n",
    "def select_important_features(model, X_train, X_test, threshold='median'):\n",
    "    \"\"\"Select most important features using trained model\"\"\"\n",
    "    selector = SelectFromModel(model, threshold=threshold, prefit=True)\n",
    "    \n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    selected_features = X_train.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    print(f\"\\nFeature selection: {len(selected_features)} features selected from {X_train.shape[1]}\")\n",
    "    \n",
    "    return X_train_selected, X_test_selected, selected_features\n",
    "\n",
    "# ================================================================================\n",
    "# 5. VISUALIZATION AND REPORTING\n",
    "# ================================================================================\n",
    "\n",
    "def plot_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'estimators_'):  # For ensemble models\n",
    "        importances = model.estimators_[0].feature_importances_\n",
    "    else:\n",
    "        print(\"Model doesn't have feature importances\")\n",
    "        return\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False).head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(importance_df)), importance_df['importance'])\n",
    "    plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top {top_n} Most Important Features')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "def plot_predictions(y_test, predictions_dict, dates=None):\n",
    "    \"\"\"Plot actual vs predicted values for all models\"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    if dates is not None:\n",
    "        x = dates\n",
    "        plt.xlabel('Date')\n",
    "    else:\n",
    "        x = range(len(y_test))\n",
    "        plt.xlabel('Time Step')\n",
    "    \n",
    "    plt.plot(x, y_test.values, label='Actual', linewidth=2, color='black', alpha=0.7)\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    for i, (name, pred) in enumerate(predictions_dict.items()):\n",
    "        plt.plot(x, pred, label=name, linewidth=1.5, alpha=0.6, color=colors[i % len(colors)])\n",
    "    \n",
    "    plt.ylabel('Target Value')\n",
    "    plt.title('Model Predictions vs Actual Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_comparison_table(results_dict, ar2_baseline):\n",
    "    \"\"\"Create comprehensive model comparison table\"\"\"\n",
    "    comparison = []\n",
    "    \n",
    "    # Add AR(2) baseline\n",
    "    comparison.append({\n",
    "        'Model': 'AR(2) Baseline',\n",
    "        'Train R²': np.nan,\n",
    "        'Test R²': ar2_baseline['r2'],\n",
    "        'Test RMSE': ar2_baseline['rmse'],\n",
    "        'Test MAE': ar2_baseline['mae'],\n",
    "        'Overfit': np.nan,\n",
    "        'vs Baseline': 0.0\n",
    "    })\n",
    "    \n",
    "    # Add ML models\n",
    "    for name, results in results_dict.items():\n",
    "        comparison.append({\n",
    "            'Model': name,\n",
    "            'Train R²': results.get('train_r2', np.nan),\n",
    "            'Test R²': results['test_r2'],\n",
    "            'Test RMSE': results['test_rmse'],\n",
    "            'Test MAE': results['test_mae'],\n",
    "            'Overfit': results.get('train_r2', np.nan) - results['test_r2'],\n",
    "            'vs Baseline': results['test_r2'] - ar2_baseline['r2']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison)\n",
    "    df = df.sort_values('Test R²', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================================================================================\n",
    "# 6. MAIN PIPELINE\n",
    "# ================================================================================\n",
    "\n",
    "def main(filepath, target_col='price', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Main pipeline that orchestrates the entire analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to your CSV file\n",
    "    target_col : str\n",
    "        Name of target variable column\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ADVANCED ML TIME SERIES FORECASTING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    df = load_data(filepath)\n",
    "    print(f\"   Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Create features\n",
    "    print(\"\\n2. Creating advanced features...\")\n",
    "    df_features = create_advanced_features(df, target_col)\n",
    "    \n",
    "    # Remove rows with NaN (from lagging)\n",
    "    df_features = df_features.dropna()\n",
    "    print(f\"   Created {len(df_features.columns) - len(df.columns)} new features\")\n",
    "    print(f\"   Rows after lagging: {len(df_features)}\")\n",
    "    \n",
    "    # Prepare train/test split\n",
    "    split_idx = int(len(df_features) * (1 - test_size))\n",
    "    \n",
    "    feature_cols = [col for col in df_features.columns if col not in ['date', target_col]]\n",
    "    \n",
    "    X = df_features[feature_cols]\n",
    "    y = df_features[target_col]\n",
    "    \n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    dates_test = df_features['date'].iloc[split_idx:] if 'date' in df_features.columns else None\n",
    "    \n",
    "    print(f\"   Training samples: {len(X_train)}\")\n",
    "    print(f\"   Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Train baseline AR(2)\n",
    "    print(\"\\n3. Training AR(2) baseline...\")\n",
    "    ar2_results = train_ar2_baseline(y_train, y_test)\n",
    "    print(f\"   Test R²: {ar2_results['r2']:.4f}\")\n",
    "    print(f\"   Test RMSE: {ar2_results['rmse']:.4f}\")\n",
    "    \n",
    "    # Train ML models\n",
    "    print(\"\\n4. Training advanced ML models...\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    rf_results = train_random_forest_tuned(X_train, y_train, X_test, y_test, n_iter=20)\n",
    "    print(f\"   Random Forest - Test R²: {rf_results['test_r2']:.4f}\")\n",
    "    print(f\"   Best params: {rf_results['best_params']}\")\n",
    "    \n",
    "    gb_results = train_gradient_boosting_tuned(X_train, y_train, X_test, y_test)\n",
    "    print(f\"   Gradient Boosting - Test R²: {gb_results['test_r2']:.4f}\")\n",
    "    print(f\"   Best params: {gb_results['best_params']}\")\n",
    "    \n",
    "    # Create ensemble\n",
    "    print(\"\\n5. Creating ensemble model...\")\n",
    "    ensemble_results = train_ensemble(\n",
    "        rf_results['model'], \n",
    "        gb_results['model'],\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    print(f\"   Ensemble - Test R²: {ensemble_results['test_r2']:.4f}\")\n",
    "    \n",
    "    # Model comparison\n",
    "    print(\"\\n6. Model Comparison\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results_dict = {\n",
    "        'Random Forest': rf_results,\n",
    "        'Gradient Boosting': gb_results,\n",
    "        'Ensemble': ensemble_results\n",
    "    }\n",
    "    \n",
    "    comparison_df = create_comparison_table(results_dict, ar2_results)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Feature importance\n",
    "    print(\"\\n7. Feature Importance Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_model_name = comparison_df.iloc[1]['Model']  # Second row (first is baseline)\n",
    "    best_model = results_dict[best_model_name]['model']\n",
    "    \n",
    "    importance_df = plot_feature_importance(best_model, feature_cols, top_n=20)\n",
    "    print(\"\\nTop 10 features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Predictions plot\n",
    "    print(\"\\n8. Generating prediction plots...\")\n",
    "    predictions_dict = {\n",
    "        'AR(2)': ar2_results['predictions'],\n",
    "        'Random Forest': rf_results['predictions'],\n",
    "        'Gradient Boosting': gb_results['predictions'],\n",
    "        'Ensemble': ensemble_results['predictions']\n",
    "    }\n",
    "    \n",
    "    plot_predictions(y_test, predictions_dict, dates_test)\n",
    "    \n",
    "    # Final recommendations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_r2 = comparison_df.iloc[1]['Test R²']\n",
    "    improvement = comparison_df.iloc[1]['vs Baseline']\n",
    "    \n",
    "    if best_r2 > ar2_results['r2']:\n",
    "        print(f\"✓ Use {best_model_name} for forecasting\")\n",
    "        print(f\"  Improvement over AR(2): {improvement:.4f} ({improvement/ar2_results['r2']*100:.1f}%)\")\n",
    "        print(f\"  Test R²: {best_r2:.4f}\")\n",
    "    else:\n",
    "        print(\"✗ ML models did not beat baseline - stick with AR(2)\")\n",
    "    \n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Monitor model performance over time\")\n",
    "    print(\"  2. Retrain quarterly with new data\")\n",
    "    print(\"  3. Set up prediction intervals for risk management\")\n",
    "    \n",
    "    return {\n",
    "        'models': results_dict,\n",
    "        'comparison': comparison_df,\n",
    "        'feature_importance': importance_df,\n",
    "        'test_data': (X_test, y_test),\n",
    "        'best_model': best_model\n",
    "    }\n",
    "\n",
    "# ================================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Modify these parameters for your data\n",
    "    FILEPATH = 'your_data.csv'  # Change to your file path\n",
    "    TARGET_COL = 'price'        # Change to your target column name\n",
    "    TEST_SIZE = 0.2             # 20% test set\n",
    "    \n",
    "    # Run pipeline\n",
    "    results = main(\n",
    "        filepath=FILEPATH,\n",
    "        target_col=TARGET_COL,\n",
    "        test_size=TEST_SIZE\n",
    "    )\n",
    "    \n",
    "    # Access results\n",
    "    best_model = results['best_model']\n",
    "    comparison_table = results['comparison']\n",
    "    feature_importance = results['feature_importance']\n",
    "    \n",
    "    # Make future predictions (example)\n",
    "    # X_future = create_features_for_future_data(...)\n",
    "    # predictions = best_model.predict(X_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "ensemble = VotingRegressor([\n",
    "    ('rf', tuned_rf),\n",
    "    ('gb', tuned_gb)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
